{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc171362",
   "metadata": {},
   "source": [
    "# Parte 3: Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04835374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.preprocessing import TargetEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c059e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'data/train.csv'\n",
    "df = pd.read_csv(train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999798d7",
   "metadata": {},
   "source": [
    "## 1) Feature engineering (común a ambos modelos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpieza y preprocesamiento básico de texto\n",
    "def clean_text(s):\n",
    "    if pd.isna(s):\n",
    "        return ''\n",
    "    s = str(s)\n",
    "    s = s.lower()\n",
    "    s = re.sub(r'http\\S+', ' ', s)\n",
    "    s = re.sub(r'www\\S+', ' ', s)\n",
    "    s = re.sub(r'[^\\w\\s#@]', ' ', s)\n",
    "    s = re.sub(r'[\\s_]+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "df['text_clean'] = df['text'].apply(clean_text)\n",
    "df['keyword'] = df['keyword'].fillna('no_keyword_contained')\n",
    "df['location'] = df['location'].apply(clean_text)\n",
    "df['location'] = df['location'].fillna('no_location_contained')\n",
    "\n",
    "# Creación de features numéricas adicionales\n",
    "df['word_count'] = df['text_clean'].apply(lambda s: len(s.split()))\n",
    "df['text_len'] = df['text_clean'].apply(lambda s: sum(len(w) for w in s.split()))\n",
    "df['mean_word_len'] = df.apply(lambda row: row['text_len'] / row['word_count'] if row['word_count'] > 0 else 0, axis=1)\n",
    "df['num_hashtags'] = df['text'].apply(lambda s: 0 if pd.isna(s) else s.count('#'))\n",
    "df['num_mentions'] = df['text'].apply(lambda s: 0 if pd.isna(s) else s.count('@'))\n",
    "\n",
    "# Creación de features booleanas adicionales\n",
    "df['has_url'] = df['text'].apply(lambda s: 0 if pd.isna(s) else (1 if 'http' in s or 'www.' in s else 0))\n",
    "df['has_hashtag'] = df['num_hashtags'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df['has_mention'] = df['num_mentions'].apply(lambda x: 1 if x > 0 else 0)\n",
    "df['location_mentioned'] = df.apply(lambda row: 1 if row['location'].lower() in row['text_clean'] else 0, axis=1)\n",
    "\n",
    "disaster_terms = df['keyword'].dropna().unique().tolist()\n",
    "def count_terms(s, terms=disaster_terms):\n",
    "    s = s.lower()\n",
    "    cnt = 0\n",
    "    for t in terms:\n",
    "        if t in s:\n",
    "            cnt += 1\n",
    "    return cnt\n",
    "\n",
    "# Feature engineering adicional \n",
    "df['disaster_terms_count'] = df['text_clean'].apply(count_terms)\n",
    "df['all_caps_count'] = df['text'].apply(lambda s: sum(1 for w in str(s).split() if w.isupper())).fillna(0)\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_scores(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return {'neg': 0.0, 'neu': 0.0, 'pos': 0.0, 'compound': 0.0}\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "scores = [vader_scores(t) for t in df['text'].astype(str).tolist()]\n",
    "scores_df = pd.DataFrame(scores)\n",
    "\n",
    "df = pd.concat([df.reset_index(drop=True), scores_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b899d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_appearings = df['location'].value_counts().to_dict()\n",
    "df['location_clean'] = df['location'].map(lambda loc: loc if location_appearings[loc] > 2 else \"other\")\n",
    "\n",
    "keyword_counts = df['keyword'].value_counts()\n",
    "kw_Q1 = keyword_counts.quantile(0.25)\n",
    "low_frequency_keywords = keyword_counts[keyword_counts < kw_Q1].to_dict()\n",
    "df['keyword_clean'] = df['keyword'].map(lambda kw: kw if kw not in low_frequency_keywords else \"other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = ['text_len', 'word_count', 'mean_word_len',\n",
    "                'num_hashtags', 'num_mentions', 'disaster_terms_count',\n",
    "                'all_caps_count', 'neg', 'neu', 'pos', 'compound']\n",
    "cat_features = ['location_clean', 'keyword_clean']\n",
    "bool_features = ['has_url', 'has_hashtag', 'has_mention', 'location_mentioned']\n",
    "emb_features = ['text_clean']\n",
    "X = df[num_features + cat_features + bool_features + emb_features]\n",
    "y = df['target'].astype(int).values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cb7753",
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_encoding = TargetEncoder(random_state=SEED)\n",
    "loc_encoding.fit(X_train[['location_clean']], y_train)\n",
    "X_train_loc_enc = loc_encoding.transform(X_train[['location_clean']])\n",
    "X_val_loc_enc = loc_encoding.transform(X_val[['location_clean']])\n",
    "\n",
    "kyw_encoding = TargetEncoder(random_state=SEED)\n",
    "kyw_encoding.fit(X_train[['keyword_clean']], y_train)\n",
    "X_train_kyw_enc = kyw_encoding.transform(X_train[['keyword_clean']])\n",
    "X_val_kyw_enc = kyw_encoding.transform(X_val[['keyword_clean']])\n",
    "\n",
    "X_train['location_enc'] = X_train_loc_enc\n",
    "X_train['keyword_enc'] = X_train_kyw_enc\n",
    "X_val['location_enc'] = X_val_loc_enc\n",
    "X_val['keyword_enc'] = X_val_kyw_enc\n",
    "num_features += ['location_enc', 'keyword_enc']\n",
    "\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', StandardScaler(), num_features),\n",
    "    ('bool', 'passthrough', bool_features)\n",
    "], remainder='drop')\n",
    "\n",
    "X_train_proc = preprocessor.fit_transform(X_train[num_features + bool_features])\n",
    "X_val_proc = preprocessor.transform(X_val[num_features + bool_features])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d18f60",
   "metadata": {},
   "source": [
    "## Primer modelo: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05107ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecebbeb",
   "metadata": {},
   "source": [
    "### KNN sin escalar features numéricas (y sin embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c0c3cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 22 candidates, totalling 110 fits\n",
      "Mejores parámetros: {'n_neighbors': 8, 'weights': 'distance'}\n",
      "Mejor puntaje F1 en Cross Validation: 0.615239877661397\n",
      "F1 en validación: 0.6285714285714286\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       869\n",
      "           1       0.65      0.61      0.63       654\n",
      "\n",
      "    accuracy                           0.69      1523\n",
      "   macro avg       0.69      0.68      0.68      1523\n",
      "weighted avg       0.69      0.69      0.69      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,5,7,8,9,10,20,30,50,100,150],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "grid = GridSearchCV(knn, param_grid, scoring='f1', cv=cv, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(X_train[num_features + bool_features], y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid.best_params_)\n",
    "print(\"Mejor puntaje F1 en Cross Validation:\", grid.best_score_)\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "y_val_pred = best_knn.predict(X_val[num_features + bool_features])\n",
    "print(\"F1 en validación:\", f1_score(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e017ba46",
   "metadata": {},
   "source": [
    "### KNN Escalando features numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "805db21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 26 candidates, totalling 130 fits\n",
      "Mejores parámetros: {'n_neighbors': 181, 'weights': 'distance'}\n",
      "Mejor puntaje F1 en Cross Validation: 0.7056738974906606\n",
      "F1 en validación: 0.7116666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81       869\n",
      "           1       0.78      0.65      0.71       654\n",
      "\n",
      "    accuracy                           0.77      1523\n",
      "   macro avg       0.77      0.76      0.76      1523\n",
      "weighted avg       0.77      0.77      0.77      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [50,100,150,175,178,180,181,182,183,185,200,500,1000],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "grid = GridSearchCV(knn, param_grid, scoring='f1', cv=cv, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(X_train_proc, y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid.best_params_)\n",
    "print(\"Mejor puntaje F1 en Cross Validation:\", grid.best_score_)\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "y_val_pred = best_knn.predict(X_val_proc)\n",
    "print(\"F1 en validación:\", f1_score(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59543c93",
   "metadata": {},
   "source": [
    "### KNN Escalando features numéricas + TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e93dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=100, stop_words='english')\n",
    "vectorizer.fit(X_train['text_clean'])\n",
    "X_train_tfidf = vectorizer.transform(X_train['text_clean']).toarray()\n",
    "X_val_tfidf = vectorizer.transform(X_val['text_clean']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc45bbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n",
      "Mejores parámetros: {'metric': 'cosine', 'n_neighbors': 22, 'weights': 'distance'}\n",
      "Mejor puntaje F1 en Cross Validation: 0.7154319868702279\n",
      "F1 en validación: 0.7125506072874493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.84      0.80       869\n",
      "           1       0.76      0.67      0.71       654\n",
      "\n",
      "    accuracy                           0.77      1523\n",
      "   macro avg       0.77      0.76      0.76      1523\n",
      "weighted avg       0.77      0.77      0.76      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,5,10,13,14,15,16,17,18,19,20,21,22,23,25,30,50,100,500],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine', 'minkowski'],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "grid = GridSearchCV(knn, param_grid, scoring='f1', cv=cv, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(np.hstack((X_train_proc, X_train_tfidf)), y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid.best_params_)\n",
    "print(\"Mejor puntaje F1 en Cross Validation:\", grid.best_score_)\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "y_val_pred = best_knn.predict(np.hstack((X_val_proc, X_val_tfidf)))\n",
    "print(\"F1 en validación:\", f1_score(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09a34c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18637b7b",
   "metadata": {},
   "source": [
    "### KNN Escalando features numéricas + Word2Vec preentrenado (300 dimensiones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "87525fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "w2v_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(w2v_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0a415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 300)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pattern = re.compile(r\"\\w+\")\n",
    "def tokenize(text):\n",
    "    return token_pattern.findall(text)\n",
    "\n",
    "def sentence_vector_avg(tokens, model, dim):\n",
    "    if not tokens:\n",
    "        return np.zeros(dim, dtype=float)\n",
    "    vecs = []\n",
    "    for w in tokens:\n",
    "        if w in model:\n",
    "            vecs.append(model[w])\n",
    "        elif w.lower() in model:\n",
    "            vecs.append(model[w.lower()])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(dim, dtype=float)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "X_train_tokenized =  [tokenize(t) for t in X_train['text_clean'].astype(str).tolist()]\n",
    "X_val_tokenized =  [tokenize(t) for t in X_val['text_clean'].astype(str).tolist()]\n",
    "\n",
    "vecs = []\n",
    "labels = []\n",
    "for i in range(len(X_train_tokenized)):\n",
    "    tokens = X_train_tokenized[i]\n",
    "    v = sentence_vector_avg(tokens, w2v, dim=w2v.vector_size)\n",
    "    vecs.append(v)\n",
    "    labels.append(int(df.iloc[i]['target']))\n",
    "\n",
    "w2v_emb = np.vstack(vecs)\n",
    "w2v_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f62cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': [1,5,10,13,14,15,16,17,18,19,20,21,22,23,25,30,50,100,500],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'cosine', 'minkowski'],\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "grid = GridSearchCV(knn, param_grid, scoring='f1', cv=cv, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid.fit(np.hstack((X_train_proc, X_train_w2v)), y_train)\n",
    "\n",
    "print(\"Mejores parámetros:\", grid.best_params_)\n",
    "print(\"Mejor puntaje F1 en Cross Validation:\", grid.best_score_)\n",
    "\n",
    "best_knn = grid.best_estimator_\n",
    "\n",
    "y_val_pred = best_knn.predict(np.hstack((X_val_proc, X_val_w2v)))\n",
    "print(\"F1 en validación:\", f1_score(y_val, y_val_pred))\n",
    "print(classification_report(y_val, y_val_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
